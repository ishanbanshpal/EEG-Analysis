{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7e3657f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the path to the info file (or leave blank if not available): \n",
      "Enter the path to the EEG file: c_rest_n6.txt\n",
      "Enter the EEG state (meditation or rest): rest\n",
      "Enter the Level (experienced or novice): novice\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1793205669.py:25: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_without_headers = pd.read_csv(file_path, delimiter=delimiter, encoding='latin1', on_bad_lines='skip', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sampling frequency: 512\n",
      "Creating RawArray with float64 data, n_channels=24, n_times=620546\n",
      "    Range : 0 ... 620545 =      0.000 ...  1212.002 secs\n",
      "Ready.\n",
      "<RawArray | 24 x 620546 (1212.0 s), ~113.6 MB, data loaded>\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 4 - 1e+02 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 4.00\n",
      "- Lower transition bandwidth: 2.00 Hz (-6 dB cutoff frequency: 3.00 Hz)\n",
      "- Upper passband edge: 100.00 Hz\n",
      "- Upper transition bandwidth: 25.00 Hz (-6 dB cutoff frequency: 112.50 Hz)\n",
      "- Filter length: 845 samples (1.650 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter from 59 - 61 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 59.35\n",
      "- Lower transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 59.10 Hz)\n",
      "- Upper passband edge: 60.65 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-6 dB cutoff frequency: 60.90 Hz)\n",
      "- Filter length: 3381 samples (6.604 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel names in the dataset:\n",
      "['FP1', 'FP2', 'F7', 'F3', 'Fz', 'F4', 'F8', 'T3', 'C3', 'Cz', 'C4', 'T4', 'T5', 'P3', 'PZ', 'P4', 'T6', 'O1', 'O2', 'A1', 'A2', 'SensorCEOG', 'SensorDEOG', 'Events']\n",
      "Enter the path to the info file (or leave blank if not available): \n",
      "Error setting montage: DigMontage is only a subset of info. There are 6 channel positions not present in the DigMontage. The channels missing from the montage are:\n",
      "\n",
      "['FP1', 'FP2', 'PZ', 'SensorCEOG', 'SensorDEOG', 'Events'].\n",
      "\n",
      "Consider using inst.rename_channels to match the montage nomenclature, or inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "Setting montage without positions...\n",
      "Extracted info parameters: {}\n"
     ]
    }
   ],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from mne.preprocessing import ICA\n",
    "import pandas as pd\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# Ask for info file input\n",
    "info_file_path = input(\"Enter the path to the info file (or leave blank if not available): \")\n",
    "\n",
    "def read_eeg_data(file_path, info_file_path=None):\n",
    "    _, file_extension = os.path.splitext(file_path)\n",
    "    file_extension = file_extension.lower()\n",
    "    \n",
    "    if file_extension == '.edf':\n",
    "        return mne.io.read_raw_edf(file_path, preload=True)\n",
    "    elif file_extension == '.set':\n",
    "        return mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "    elif file_extension in ['.txt', '.csv', '.xlsx']:\n",
    "        delimiter = ','\n",
    "        # Read the data without headers to infer data types\n",
    "        df_without_headers = pd.read_csv(file_path, delimiter=delimiter, encoding='latin1', on_bad_lines='skip', header=None)\n",
    "        \n",
    "        channel_names = df_without_headers.iloc[0].tolist()\n",
    "        # Infer data types for each column\n",
    "        dtype_dict = df_without_headers.dtypes.to_dict()\n",
    "        # Read the data again with headers and specified data types\n",
    "        df = pd.read_csv(file_path, delimiter=delimiter, encoding='latin1', on_bad_lines='skip', dtype=dtype_dict)\n",
    "        # Set the first row as column names\n",
    "        df.columns = df.iloc[0]\n",
    "        # Drop the first row\n",
    "        df = df.drop(0)\n",
    "        # Convert all columns to numeric except for the first row (which is now the column names)\n",
    "        df = df.apply(pd.to_numeric, errors='coerce')\n",
    "        # Fill NaN values with 0 and then fill with column averages\n",
    "        column_averages = df.mean()\n",
    "        df.fillna(0, inplace=True)\n",
    "        df.fillna(column_averages, inplace=True)\n",
    "\n",
    "        # Assuming the data is in columns and each row represents a sample\n",
    "        data = df.to_numpy().T   # Transpose to have channels as rows and samples as columns\n",
    "        sfreq = None\n",
    "        \n",
    "        if info_file_path:\n",
    "            info_params = extract_info_file_params(info_file_path)\n",
    "            sfreq = info_params.get('sampling_rate')\n",
    "        if not sfreq:\n",
    "            sfreq = float(input(\"Enter the sampling frequency: \"))\n",
    "            \n",
    "        \n",
    "        # Convert non-numeric values to 0\n",
    "        df = df.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        \n",
    "        # Calculate column averages\n",
    "        column_averages = df.mean()\n",
    "        \n",
    "        # Fill NaN values with column averages\n",
    "        df.fillna(column_averages, inplace=True)\n",
    "        \n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].astype(str)\n",
    "            df[column] = df[column].str.replace('[^\\d.-]', '', regex=True)\n",
    "            df[column] = df[column].str.replace('\\.{2,}', '.', regex=True)\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0)\n",
    "           \n",
    "        column_averages=df.mean()\n",
    "        for column in df.columns:\n",
    "            df[column].fillna(column_averages[column], inplace=True)\n",
    "\n",
    "               \n",
    "        \n",
    "        info = mne.create_info(ch_names=channel_names, sfreq=sfreq, ch_types='eeg')\n",
    "        raw = mne.io.RawArray(data, info)\n",
    "        return raw\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {file_extension}\")\n",
    "\n",
    "# Ask for EEG file input\n",
    "eeg_file_path = input(\"Enter the path to the EEG file: \")\n",
    "\n",
    "while True:\n",
    "    eeg_state = input(\"Enter the EEG state (meditation or rest): \").lower()\n",
    "    if eeg_state in [\"meditation\", \"rest\"]:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter either 'meditation' or 'rest'.\")\n",
    "\n",
    "# Map EEG state to label\n",
    "eeg_state_label = \"Meditation\" if eeg_state == \"meditation\" else \"Rest\"\n",
    "\n",
    "while True:\n",
    "    eeg_level = input(\"Enter the Level (experienced or novice): \").lower()\n",
    "    if eeg_level in [\"experienced\", \"novice\"]:\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid input. Please enter either 'experienced' or 'novice'.\")\n",
    "\n",
    "# Map EEG state to label\n",
    "eeg_level_label = \"Experienced\" if eeg_level == \"experienced\" else \"Novice\"\n",
    "\n",
    "\n",
    "\n",
    "# Load EEG data\n",
    "raw_data = read_eeg_data(eeg_file_path)\n",
    "\n",
    "print(raw_data)\n",
    "\n",
    "# Apply bandpass filtering\n",
    "raw_data.filter(l_freq=4, h_freq=100, picks='eeg')\n",
    "\n",
    "# Apply notch filtering (e.g., remove 60 Hz powerline interference)\n",
    "raw_data.notch_filter(freqs=60, picks='all')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Show all channel names in the dataset\n",
    "print(\"Channel names in the dataset:\")\n",
    "print(raw_data.info['ch_names'])\n",
    "\n",
    "# Map channel names to the appropriate nomenclature\n",
    "channel_mapping = {\n",
    "    'Fp1': 'Fp1', 'Fp2': 'Fp2', 'F3': 'F3', 'F4': 'F4', 'C3': 'C3',\n",
    "    'C4': 'C4', 'P3': 'P3', 'P4': 'P4', 'O1': 'O1', 'O2': 'O2',\n",
    "    'F7': 'F7', 'F8': 'F8', 'T7': 'T7', 'T8': 'T8', 'P7': 'P7',\n",
    "    'P8': 'P8', 'Fz': 'Fz', 'Cz': 'Cz', 'Pz': 'Pz', 'FC1': 'FC1',\n",
    "    'FC2': 'FC2', 'CP1': 'CP1', 'CP2': 'CP2', 'FC5': 'FC5', 'FC6': 'FC6',\n",
    "    'CP5': 'CP5', 'CP6': 'CP6', 'FT9': 'FT9', 'FT10': 'FT10',\n",
    "    'TP9': 'TP9', 'TP10': 'TP10', 'POz': 'POz', 'ECG': 'ECG',\n",
    "    'EXG1': 'EXG1', 'EXG2': 'EXG2', 'EXG3': 'EXG3', 'EXG4': 'EXG4',\n",
    "    'EXG5': 'EXG5', 'EXG6': 'EXG6', 'EXG7': 'EXG7', 'EXG8': 'EXG8',\n",
    "    'Status': 'Status'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Ask for info file input\n",
    "info_file_path = input(\"Enter the path to the info file (or leave blank if not available): \")\n",
    "\n",
    "def extract_info_file_params(info_file_paAth):\n",
    "    \"\"\"\n",
    "    Extract parameters from the info file.\n",
    "\n",
    "    Args:\n",
    "    info_file_path (str): Path to the info file.\n",
    "\n",
    "    Returns:\n",
    "    dict: Dictionary containing extracted parameters.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    if info_file_path:\n",
    "        with open(info_file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            line = line.strip()  # Remove leading and trailing whitespace\n",
    "            if line.startswith('Total number of channels:'):\n",
    "                params['total_channels'] = int(line.split(':')[1].strip())\n",
    "            elif line.startswith('EEG sampling rate:'):\n",
    "                params['sampling_rate'] = float(line.split(':')[1].split()[0])\n",
    "            elif line.startswith('Number of EEG channels:'):\n",
    "                params['num_eeg_channels'] = int(line.split(':')[1].strip())\n",
    "            elif line.startswith('Accelerometer data:'):\n",
    "                params['accelerometer_data'] = line.split(':')[1].strip() == 'ON'\n",
    "            elif line.startswith('Accelerometer sampling rate:'):\n",
    "                params['accelerometer_sampling_rate'] = float(line.split(':')[1].strip().split()[0])\n",
    "            elif line.startswith('Number of channels of Accelerometer:'):\n",
    "                params['num_accelerometer_channels'] = int(line.split(':')[1].strip())\n",
    "            elif line.startswith('Reference channels:'):\n",
    "                params['reference_channels'] = int(line.split(':')[1].strip())\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "sampling_rate = None\n",
    "accelerometer_data = False\n",
    "accelerometer_sampling_rate = None\n",
    "accelerometer_units = None\n",
    "if info_file_path:\n",
    "    info_params = extract_info_file_params(info_file_path)\n",
    "    sampling_rate = info_params.get('sampling_rate')\n",
    "    accelerometer_data = info_params.get('accelerometer_data', False)\n",
    "    accelerometer_sampling_rate = info_params.get('accelerometer_sampling_rate')\n",
    "    accelerometer_units = info_params.get('accelerometer_units')\n",
    "    num_accelerometer_channels = info_params.get('num_accelerometer_channels')\n",
    "    \n",
    "\n",
    "# Set montage according to info file or user preference\n",
    "if info_file_path:\n",
    "    info_params = extract_info_file_params(info_file_path)\n",
    "    montage = info_params.get('montage_channels')\n",
    "    if not montage:\n",
    "        montage_choice = input(\"Montage not found in info file. Do you want to use 10-10 montage? (y/n): \")\n",
    "        if montage_choice.lower() == 'y':\n",
    "            montage = 'standard_1005'\n",
    "        else:\n",
    "            montage = 'standard_1020'\n",
    "else:\n",
    "    montage = 'standard_1020'\n",
    "\n",
    "if montage:\n",
    "    try:\n",
    "        montage = mne.channels.make_standard_montage(montage, head_size=1.0)\n",
    "        raw_data.set_montage(montage)\n",
    "    except ValueError as e:\n",
    "        print(\"Error setting montage:\", e)\n",
    "        print(\"Setting montage without positions...\")\n",
    "        raw_data.set_montage(montage, on_missing='ignore')\n",
    "else:\n",
    "    print(\"Montage not specified. Setting default 10-20 montage.\")\n",
    "    montage = 'standard_1020'\n",
    "    raw_data.set_montage(montage, on_missing='ignore')\n",
    "\n",
    "\n",
    "    \n",
    "info_params = extract_info_file_params(info_file_path)\n",
    "print(\"Extracted info parameters:\", info_params)  # Move this line here    \n",
    "    \n",
    "    #meditation    #rest    #novice "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73292fd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel names in the dataset:\n",
      "  1. FP1\n",
      "  2. FP2\n",
      "  3. F7\n",
      "  4. F3\n",
      "  5. Fz\n",
      "  6. F4\n",
      "  7. F8\n",
      "  8. T3\n",
      "  9. C3\n",
      "  10. Cz\n",
      "  11. C4\n",
      "  12. T4\n",
      "  13. T5\n",
      "  14. P3\n",
      "  15. PZ\n",
      "  16. P4\n",
      "  17. T6\n",
      "  18. O1\n",
      "  19. O2\n",
      "  20. A1\n",
      "  21. A2\n",
      "  22. SensorCEOG\n",
      "  23. SensorDEOG\n",
      "  24. Events\n",
      "Do you want to remove any channels? (y/n): y\n",
      "Enter the channels to remove (separated by commas): F3,F4,F7,F8,T5,T6,PZ,Fz,Cz,Events,C3,C4,O1,O2\n",
      "Do you want to edit any channel names? (y/n): n\n",
      "Reference channels not found in the info file.\n",
      "Available channel names:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1793205669.py:25: DtypeWarning: Columns (0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_without_headers = pd.read_csv(file_path, delimiter=delimiter, encoding='latin1', on_bad_lines='skip', header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the sampling frequency: 512\n",
      "Creating RawArray with float64 data, n_channels=24, n_times=620546\n",
      "    Range : 0 ... 620545 =      0.000 ...  1212.002 secs\n",
      "Ready.\n",
      "  1: FP1\n",
      "  2: FP2\n",
      "  3: F7\n",
      "  4: F3\n",
      "  5: Fz\n",
      "  6: F4\n",
      "  7: F8\n",
      "  8: T3\n",
      "  9: C3\n",
      "  10: Cz\n",
      "  11: C4\n",
      "  12: T4\n",
      "  13: T5\n",
      "  14: P3\n",
      "  15: PZ\n",
      "  16: P4\n",
      "  17: T6\n",
      "  18: O1\n",
      "  19: O2\n",
      "  20: A1\n",
      "  21: A2\n",
      "  22: SensorCEOG\n",
      "  23: SensorDEOG\n",
      "  24: Events\n",
      "Enter the names of the reference channels (comma-separated): A1,A2\n",
      "Selected reference channels: ['A1', 'A2']\n",
      "Reference channels detected. Applying average re-referencing.\n",
      "EEG channel type selected for re-referencing\n",
      "Applying average reference.\n",
      "Applying a custom ('EEG',) reference.\n"
     ]
    }
   ],
   "source": [
    "#20240503172253_ShashankT_Meditation.info\n",
    "# Show all channel names in the dataset according to nomenclature\n",
    "print(\"Channel names in the dataset:\")\n",
    "for idx, channel in enumerate(raw_data.info['ch_names']):\n",
    "    print(f\"  {idx + 1}. {channel_mapping.get(channel, channel)}\")\n",
    "\n",
    "\n",
    "info_params = extract_info_file_params(info_file_path)\n",
    "num_accelerometer_channels = info_params.get('num_accelerometer_channels')\n",
    "# Ask for the names of accelerometer channels available in the raw EDF file\n",
    "accelerometer_channel_names = []\n",
    "if num_accelerometer_channels:\n",
    "    print(f\"Found {num_accelerometer_channels} accelerometer channels in the info file.\")\n",
    "    for i in range(num_accelerometer_channels):\n",
    "        channel_name = input(f\"Enter the name of accelerometer channel {i+1}: \")\n",
    "        accelerometer_channel_names.append(channel_name)\n",
    "\n",
    "# Ask if any channels need to be removed\n",
    "remove_channels = input(\"Do you want to remove any channels? (y/n): \")\n",
    "if remove_channels.lower() == 'y':\n",
    "    # Ask for channels to remove\n",
    "    channels_to_remove = input(\"Enter the channels to remove (separated by commas): \").split(',')\n",
    "else:\n",
    "    channels_to_remove = []\n",
    "\n",
    "eeg_channels = [ch for ch in raw_data.ch_names if ch not in channels_to_remove and ch not in accelerometer_channel_names]\n",
    "\n",
    "\n",
    "# Ask if the user wants to edit channel names\n",
    "edit_channel_names = input(\"Do you want to edit any channel names? (y/n): \")\n",
    "if edit_channel_names.lower() == 'y':\n",
    "    while True:\n",
    "        channel_to_edit = input(\"Enter the channel name you want to edit (leave blank to continue): \")\n",
    "        if not channel_to_edit:\n",
    "            break\n",
    "        new_name = input(f\"Enter the new name for {channel_to_edit}: \")\n",
    "        raw_data.info['ch_names'][raw_data.info['ch_names'].index(channel_to_edit)] = new_name\n",
    "\n",
    "# Detect reference channels\n",
    "reference_channels = []\n",
    "if info_file_path:\n",
    "    info_params = extract_info_file_params(info_file_path)\n",
    "    reference_channels_info = info_params.get('reference_channels')\n",
    "    if reference_channels_info:\n",
    "        reference_channels = reference_channels_info.split(',')\n",
    "\n",
    "if not reference_channels:\n",
    "    print(\"Reference channels not found in the info file.\")\n",
    "    print(\"Available channel names:\")\n",
    "    raw_data = read_eeg_data(eeg_file_path)\n",
    "    for idx, channel in enumerate(raw_data.info['ch_names']):\n",
    "        print(f\"  {idx + 1}: {channel}\")\n",
    "    while True:\n",
    "        reference_channel_names = input(\"Enter the names of the reference channels (comma-separated): \")\n",
    "        if reference_channel_names:\n",
    "            reference_channel_names = [name.strip() for name in reference_channel_names.split(',')]\n",
    "            if all(name in raw_data.info['ch_names'] for name in reference_channel_names):\n",
    "                reference_channels = reference_channel_names\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid input. Please enter valid channel names (e.g., Fp1, Fp2, Cz).\")\n",
    "        else:\n",
    "            print(\"No reference channels provided. Skipping re-referencing.\")\n",
    "            reference_channels = []\n",
    "            break\n",
    "\n",
    "print(\"Selected reference channels:\", reference_channels)\n",
    "\n",
    "# Check if reference channels are present in the data\n",
    "if reference_channels:\n",
    "    print(\"Reference channels detected. Applying average re-referencing.\")\n",
    "    raw_data.set_eeg_reference(ref_channels='average', projection=False)\n",
    "    # Remove reference channels from the raw data\n",
    "    raw_data.drop_channels(reference_channels)\n",
    "else:\n",
    "    print(\"No reference channels detected. Keeping original referencing.\")\n",
    "\n",
    "\n",
    "\n",
    "eeg_channels = [ch for ch in raw_data.ch_names if ch not in channels_to_remove and ch not in accelerometer_channel_names and ch not in reference_channel_names]\n",
    "all_channels = eeg_channels + accelerometer_channel_names + reference_channels\n",
    "not_eeg_channels = [ch for ch in raw_data.ch_names if ch not in eeg_channels]\n",
    "\n",
    "\n",
    "#T3,T4,T5,T6,P3,P4,O1,O2,C3,C4,Events,PZ,Fz,Cz,FP1,FP2T3,T4,T5,T6,P3,P4,O1,O2,C3,C4,Events,PZ,Fz,Cz,FP1,FP2\n",
    "\n",
    "#T3,T4,T5,T6,P3,P4,O1,O2,C3,C4,Events,PZ,Fz,Cz,FP1,FP2\n",
    "#F3,F4,F7,F8,T5,T6,PZ,Fz,Cz,Events,C3,C4,O1,O2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8afb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to use EOG-based artifact rejection? (yes/no): yes\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Enter EOG channel names separated by commas: SensorCEOG,SensorDEOG\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "ICA with EOG data is applied.\n",
      "Fitting ICA to data using 6 channels (please be patient, this may take a while)\n",
      "Selecting by number: 2 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:23: FutureWarning: The default for pick_channels will change from ordered=False to ordered=True in 1.5 and this will result in a change of behavior because the resulting channel order will not match. Either use a channel order that matches your instance or pass ordered=False.\n",
      "  raw_data_with_eog = raw_data.copy().pick_channels(eog_channel_names + eeg_channels)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:35: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  ica.fit(raw_data_with_eeg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 1.7s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (2 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 6 PCA components\n",
      "Fitting ICA to data using 6 channels (please be patient, this may take a while)\n",
      "Selecting by number: 3 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:41: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  snr = 10 * np.log10(signal_powers / noise_powers)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:35: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  ica.fit(raw_data_with_eeg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 27.1s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (3 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 6 PCA components\n",
      "Fitting ICA to data using 6 channels (please be patient, this may take a while)\n",
      "Selecting by number: 4 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\sklearn\\decomposition\\_fastica.py:128: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:41: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  snr = 10 * np.log10(signal_powers / noise_powers)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:35: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  ica.fit(raw_data_with_eeg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 0.9s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (4 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 6 PCA components\n",
      "Fitting ICA to data using 6 channels (please be patient, this may take a while)\n",
      "Selecting by number: 5 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:41: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  snr = 10 * np.log10(signal_powers / noise_powers)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:35: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  ica.fit(raw_data_with_eeg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 23.1s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (5 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 6 PCA components\n",
      "Fitting ICA to data using 6 channels (please be patient, this may take a while)\n",
      "Selecting by number: 6 components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:41: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  snr = 10 * np.log10(signal_powers / noise_powers)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:35: RuntimeWarning: The data has not been high-pass filtered. For good ICA performance, it should be high-pass filtered (e.g., with a 1.0 Hz lower bound) before fitting ICA.\n",
      "  ica.fit(raw_data_with_eeg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting ICA took 44.6s.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (6 components)\n",
      "    Zeroing out 0 ICA components\n",
      "    Projecting back using 6 PCA components\n",
      "Using EOG channels: SensorCEOG, SensorDEOG\n",
      "... filtering ICA sources\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 5120 samples (10.000 s)\n",
      "\n",
      "... filtering target\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 5120 samples (10.000 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13620\\1306790683.py:41: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  snr = 10 * np.log10(signal_powers / noise_powers)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... filtering ICA sources\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 5120 samples (10.000 s)\n",
      "\n",
      "... filtering target\n",
      "Setting up band-pass filter from 1 - 10 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a two-pass forward and reverse, zero-phase, non-causal bandpass filter:\n",
      "- Windowed frequency-domain design (firwin2) method\n",
      "- Hann window\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 0.75 Hz)\n",
      "- Upper passband edge: 10.00 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz (-12 dB cutoff frequency: 10.25 Hz)\n",
      "- Filter length: 5120 samples (10.000 s)\n",
      "\n",
      "Artifact detection using EOG channels completed.\n",
      "Applying ICA to Raw instance\n",
      "    Transforming to ICA space (2 components)\n",
      "    Zeroing out 0 ICA components\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Projecting back using 6 PCA components\n"
     ]
    }
   ],
   "source": [
    "from mne.preprocessing import ICA\n",
    "import numpy as np\n",
    "\n",
    "snr_values = []\n",
    "best_n_components = None\n",
    "best_ica = None\n",
    "best_snr = -np.inf\n",
    "\n",
    "# Assuming raw_data, accelerometer_channel_names, eeg_channels, and channels_to_remove are defined\n",
    "\n",
    "# Ask user if they want to use EOG-based artifact rejection\n",
    "use_eog_rejection = input(\"Do you want to use EOG-based artifact rejection? (yes/no): \").lower()\n",
    "\n",
    "# Initialize eeg_raw_data\n",
    "eeg_raw_data = raw_data.copy().pick_channels(eeg_channels)\n",
    "\n",
    "if use_eog_rejection == 'yes':\n",
    "    # Your code for EOG-based artifact rejection\n",
    "    if 'EOG' in raw_data.info['ch_names']:\n",
    "        eog_indices = [raw_data.ch_names.index(ch) for ch in raw_data.info['ch_names'] if 'EOG' in ch]\n",
    "    else:\n",
    "        eog_channel_names = input(\"Enter EOG channel names separated by commas: \").split(',')\n",
    "        raw_data_with_eog = raw_data.copy().pick_channels(eog_channel_names + eeg_channels)\n",
    "        eog_indices = [raw_data_with_eog.ch_names.index(ch.strip()) for ch in eog_channel_names]\n",
    "\n",
    "    if eog_indices:\n",
    "        # Exclude EOG channels from EEG channels for ICA\n",
    "        eeg_channels_for_ica = [ch for ch in raw_data_with_eog.ch_names if ch not in eog_channel_names]\n",
    "        raw_data_with_eeg = raw_data_with_eog.copy().pick_channels(eeg_channels_for_ica)\n",
    "        \n",
    "        # Apply ICA only to EEG channels\n",
    "        print(\"ICA with EOG data is applied.\")\n",
    "        for n_components in range(2, len(raw_data_with_eeg.info['ch_names']) + 1):\n",
    "            ica = ICA(n_components=n_components)\n",
    "            ica.fit(raw_data_with_eeg)\n",
    "\n",
    "            reconstructed_data = ica.apply(raw_data_with_eeg)\n",
    "            residual = raw_data_with_eeg.get_data() - reconstructed_data.get_data()\n",
    "            noise_powers = np.mean(np.square(residual))\n",
    "            signal_powers = np.mean(np.square(reconstructed_data.get_data()))\n",
    "            snr = 10 * np.log10(signal_powers / noise_powers)\n",
    "            mean_snr = np.mean(snr)\n",
    "\n",
    "            if mean_snr > best_snr:\n",
    "                best_snr = mean_snr\n",
    "                best_n_components = n_components\n",
    "                best_ica = ica\n",
    "\n",
    "        # Discard components highly correlated with EOG channels\n",
    "        eog_threshold = 0.6\n",
    "        eog_inds, scores = ica.find_bads_eog(raw_data_with_eog, ch_name=eog_channel_names)\n",
    "        ica.exclude.extend(eog_inds)\n",
    "        for idx, score in enumerate(scores):\n",
    "            if np.any(np.abs(score) > eog_threshold):\n",
    "                ica.exclude.append(idx)\n",
    "\n",
    "        print(\"Artifact detection using EOG channels completed.\")\n",
    "\n",
    "    else:\n",
    "        print(\"No valid EOG channels found. EOG-based artifact rejection cannot be applied.\")\n",
    "\n",
    "elif accelerometer_channel_names:\n",
    "    # Check for accelerometer sampling rate\n",
    "    if 'accelerometer_sampling_rate' in raw_data.info:\n",
    "        accel_sfreq = raw_data.info['accelerometer_sampling_rate']\n",
    "    else:\n",
    "        accel_sfreq = float(input(\"Enter the sampling rate of the accelerometer data: \"))\n",
    "    \n",
    "    desired_ch = eeg_channels +accelerometer_channel_names\n",
    "    # Extract accelerometer data\n",
    "    raw_data_with_accelerometer = raw_data.copy().pick_channels(desired_ch, ordered=True)\n",
    "    accel_data = raw_data.copy().pick_channels(accelerometer_channel_names).get_data().T\n",
    "\n",
    "    # Function to identify movement periods\n",
    "    def identify_movement_periods(accel_data, sfreq, threshold=0.5):\n",
    "        # Calculate the magnitude of the acceleration vector\n",
    "        accel_magnitude = np.sqrt(np.sum(accel_data ** 2, axis=1))\n",
    "        # Identify periods where the magnitude exceeds the threshold\n",
    "        movement_periods = accel_magnitude > threshold\n",
    "        return movement_periods\n",
    "\n",
    "    # Identify movement periods\n",
    "    movement_periods = identify_movement_periods(accel_data, accel_sfreq)\n",
    "\n",
    "    # Create annotations for the movement periods\n",
    "    movement_onsets = np.where(movement_periods)[0] / accel_sfreq\n",
    "    movement_durations = np.ones_like(movement_onsets) / accel_sfreq\n",
    "    movement_descriptions = ['BAD_mov' for _ in range(len(movement_onsets))]\n",
    "\n",
    "    annotations = mne.Annotations(movement_onsets, movement_durations, movement_descriptions)\n",
    "    raw_data_with_accelerometer.set_annotations(annotations)\n",
    "\n",
    "    # Optionally, reject bad segments or interpolate bad data\n",
    "    # Rejecting bad segments:\n",
    "    raw_data_cleaned = eeg_raw_data.copy().interpolate_bads(reset_bads=True)\n",
    "\n",
    "    # Apply ICA to EEG channels with accelerometer data\n",
    "    \n",
    "    print(\"ICA with Accelerometer data is applied.\")\n",
    "    n_components_range = range(2, len(eeg_raw_data.info['ch_names']) + 1)\n",
    "    for n_components in range(2, len(eeg_raw_data.info['ch_names']) + 1):\n",
    "        ica = ICA(n_components=n_components)\n",
    "        ica.fit(eeg_raw_data)\n",
    "    \n",
    "        reconstructed_data = ica.apply(eeg_raw_data)\n",
    "        residual = eeg_raw_data.get_data() - reconstructed_data.get_data()\n",
    "        noise_powers = np.mean(np.square(residual))\n",
    "        signal_powers = np.mean(np.square(reconstructed_data.get_data()))\n",
    "        snr = 10 * np.log10(signal_powers / noise_powers)\n",
    "        mean_snr = np.mean(snr)\n",
    "\n",
    "        if mean_snr > best_snr:\n",
    "            best_snr = mean_snr\n",
    "            best_n_components = n_components\n",
    "            best_ica = ica\n",
    "\n",
    "        raw_data_cleaned = best_ica.apply(eeg_raw_data)\n",
    "    print(\"Best n_components:\", best_n_components)\n",
    "\n",
    "else:\n",
    "    # Your code for normal ICA without EOG or accelerometer\n",
    "    for n_components in range(2, len(eeg_raw_data.info['ch_names'])+1):\n",
    "        ica = ICA(n_components=n_components)\n",
    "        ica.fit(eeg_raw_data)\n",
    "\n",
    "        reconstructed_data = ica.apply(eeg_raw_data)\n",
    "        residual = eeg_raw_data.get_data() - reconstructed_data.get_data()\n",
    "        noise_powers = np.mean(np.square(residual))\n",
    "        signal_powers = np.mean(np.square(reconstructed_data.get_data()))\n",
    "        snr = 10 * np.log10(signal_powers / noise_powers)\n",
    "        mean_snr = np.mean(snr)\n",
    "\n",
    "        if mean_snr > best_snr:\n",
    "            best_snr = mean_snr\n",
    "            best_n_components = n_components\n",
    "            best_ica = ica\n",
    "        \n",
    "        raw_data_cleaned = best_ica.apply(eeg_raw_data)\n",
    "    print(\"Best n_components:\", best_n_components)\n",
    "    \n",
    "if best_ica is not None:\n",
    "    if accelerometer_channel_names:\n",
    "        # ICA with accelerometer data\n",
    "        raw_data_cleaned = best_ica.apply(eeg_raw_data)\n",
    "    elif use_eog_rejection == 'yes':\n",
    "        # ICA with EOG data\n",
    "        raw_data_cleaned = best_ica.apply(raw_data_with_eeg)\n",
    "    else:\n",
    "        raw_data_cleaned = best_ica.apply(eeg_raw_data)\n",
    "else:\n",
    "    print(\"Error: best_ica is None.\")\n",
    "#SensorCEOG,SensorDEOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b9620ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the duration of each epoch in seconds (default is 1 second): \n",
      "EEG channels selected for epoching: ['FP1', 'FP2', 'T3', 'T4', 'P3', 'P4']\n",
      "Enter the rejection threshold for epochs (leave blank for default: 100uV): \n",
      "Not setting metadata\n",
      "1212 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 1212 events and 513 original time points ...\n",
      "    Rejecting  epoch based on EEG : ['FP1', 'FP2', 'T3', 'T4', 'P3', 'P4']\n",
      "    Rejecting  epoch based on EEG : ['T3']\n",
      "    Rejecting  epoch based on EEG : ['T3']\n",
      "    Rejecting  epoch based on EEG : ['T3']\n",
      "    Rejecting  epoch based on EEG : ['T3']\n",
      "    Rejecting  epoch based on EEG : ['P3']\n",
      "    Rejecting  epoch based on EEG : ['FP1', 'FP2', 'T3', 'T4', 'P3', 'P4']\n",
      "7 bad epochs dropped\n",
      "Number of epochs after rejection: 1205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    6.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    9.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   12.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "Results appended to: Frameworkfeatures2.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Ask for the epoch duration input\n",
    "epoch_duration = float(input(\"Enter the duration of each epoch in seconds (default is 1 second): \") or 1.0)\n",
    "\n",
    "sfreq = raw_data_cleaned.info['sfreq']\n",
    "# Calculate the number of samples per epoch\n",
    "samples_per_epoch = int(epoch_duration * sfreq)\n",
    "\n",
    "# Calculate the number of epochs\n",
    "num_epochs = int(len(raw_data_cleaned) / samples_per_epoch)\n",
    "\n",
    "# Create events at the beginning of each epoch\n",
    "events = np.array([[i * samples_per_epoch, 0, 1] for i in range(num_epochs)])\n",
    "\n",
    "# Define event ID (assuming event ID is 1)\n",
    "event_id = 1\n",
    "\n",
    "# Define the time window for each epoch\n",
    "tmin = 0\n",
    "tmax = epoch_duration\n",
    "\n",
    "# Select EEG channels for epoching\n",
    "picks_eeg = mne.pick_types(raw_data_cleaned.info, meg=False, eeg=True, eog=False)\n",
    "\n",
    "# Print the names of the channels in picks_eeg\n",
    "eeg_channel_names = [raw_data_cleaned.ch_names[pick] for pick in picks_eeg]\n",
    "print(\"EEG channels selected for epoching:\", eeg_channel_names)\n",
    "\n",
    "# Ask for the rejection threshold input\n",
    "reject_threshold_input = input(\"Enter the rejection threshold for epochs (leave blank for default: 100uV): \")\n",
    "if reject_threshold_input:\n",
    "    reject_threshold = float(reject_threshold_input)\n",
    "else:\n",
    "    reject_threshold = 100  # Default threshold\n",
    "\n",
    "# Create epochs\n",
    "epochs = mne.Epochs(raw_data_cleaned, events, event_id, tmin, tmax, baseline=None,\n",
    "                    reject=dict(eeg=reject_threshold), picks=picks_eeg, preload=True,\n",
    "                    detrend=1, reject_by_annotation=False)\n",
    "\n",
    "# Check the number of remaining epochs\n",
    "num_epochs = len(epochs)\n",
    "if num_epochs == 0:\n",
    "    raise ValueError(\"All epochs were rejected. Please adjust the rejection threshold or check your data.\")\n",
    "\n",
    "print(f\"Number of epochs after rejection: {num_epochs}\")\n",
    "\n",
    "# Define frequency range and multitaper parameters\n",
    "freqs = np.arange(4, 100, 0.5)\n",
    "n_cycles = freqs / 6\n",
    "\n",
    "\n",
    "# Calculate time-frequency power\n",
    "tfr_power = tfr_multitaper(epochs, freqs=freqs, n_cycles=n_cycles,\n",
    "                           time_bandwidth=2.0, return_itc=False,\n",
    "                           picks=picks_eeg, average=False)\n",
    "\n",
    "# Extract power values\n",
    "power_values = tfr_power.data.astype(np.float32)\n",
    "\n",
    "# Define frequency ranges for PSD and band power calculation\n",
    "alpha_range = (8, 12)\n",
    "beta_range = (13, 30)\n",
    "gamma_range = (31, 100)  # Adjusted gamma range\n",
    "theta_range = (4, 7)\n",
    "\n",
    "# Calculate indices for frequency ranges\n",
    "alpha_freq_indices = np.where((freqs >= alpha_range[0]) & (freqs <= alpha_range[1]))\n",
    "theta_freq_indices = np.where((freqs >= theta_range[0]) & (freqs <= theta_range[1]))\n",
    "beta_freq_indices = np.where((freqs >= beta_range[0]) & (freqs <= beta_range[1]))\n",
    "gamma_freq_indices = np.where((freqs >= gamma_range[0]) & (freqs <= gamma_range[1]))\n",
    "\n",
    "# Initialize arrays to store calculated metrics\n",
    "num_channels = len(raw_data_cleaned.ch_names)\n",
    "slope_values = np.zeros(num_channels)\n",
    "itf_values = np.zeros(num_channels)\n",
    "iaf_values = np.zeros(num_channels)\n",
    "igf_values = np.zeros(num_channels)\n",
    "ibf_values = np.zeros(num_channels)\n",
    "iap_values = np.zeros(num_channels)\n",
    "igp_values = np.zeros(num_channels)\n",
    "ibp_values = np.zeros(num_channels)\n",
    "itp_values = np.zeros(num_channels)\n",
    "mean_psd_alpha_channels = np.zeros(num_channels)\n",
    "mean_psd_beta_channels = np.zeros(num_channels)\n",
    "mean_psd_gamma_channels = np.zeros(num_channels)\n",
    "mean_psd_theta_channels = np.zeros(num_channels)\n",
    "theta_alpha_ratio = np.zeros(num_channels)\n",
    "theta_gamma_ratio = np.zeros(num_channels)\n",
    "variance_per_channel = np.zeros(num_channels)\n",
    "entropy_per_channel =   np.zeros(num_channels)\n",
    "\n",
    "\n",
    "epsilon = 1e-10\n",
    "\n",
    "\n",
    "\n",
    "# Iterate over each channel\n",
    "for channel_idx in range (num_channels):\n",
    "    power_channel = power_values[:, channel_idx, :, :]\n",
    "        \n",
    "    \n",
    "    # Calculate PSD and band power for each frequency band\n",
    "    mean_psd_alpha_channels[channel_idx] = np.mean(np.mean(power_channel[:, alpha_freq_indices, :], axis=(0, 2)))\n",
    "    mean_psd_beta_channels[channel_idx] = np.mean(np.mean(power_channel[:, beta_freq_indices, :], axis=(0, 2)))\n",
    "    mean_psd_gamma_channels[channel_idx] = np.mean(np.mean(power_channel[:, gamma_freq_indices, :], axis=(0, 2)))\n",
    "    mean_psd_theta_channels[channel_idx] = np.mean(np.mean(power_channel[:, theta_freq_indices, :], axis=(0, 2)))\n",
    "\n",
    "    # Calculate indices of maximum power within frequency bands\n",
    "    max_power_theta_indices = np.argmax(power_channel[:, theta_freq_indices, :], axis=2)\n",
    "    max_power_alpha_indices = np.argmax(power_channel[:, alpha_freq_indices, :], axis=2)\n",
    "    max_power_beta_indices = np.argmax(power_channel[:, beta_freq_indices, :], axis=2)\n",
    "    max_power_gamma_indices = np.argmax(power_channel[:, gamma_freq_indices, :], axis=2)\n",
    "\n",
    "    # Calculate ITF, IAF, IBF, IGF values\n",
    "    itf_values[channel_idx] = np.mean(freqs[theta_freq_indices][max_power_theta_indices])\n",
    "    iaf_values[channel_idx] = np.mean(freqs[alpha_freq_indices][max_power_alpha_indices])\n",
    "    ibf_values[channel_idx] = np.mean(freqs[beta_freq_indices][max_power_beta_indices])\n",
    "    igf_values[channel_idx] = np.mean(freqs[gamma_freq_indices][max_power_gamma_indices])\n",
    "\n",
    "    # Calculate maximum power values within frequency bands\n",
    "    iap_values[channel_idx] = np.max(power_channel[:, alpha_freq_indices, :])\n",
    "    ibp_values[channel_idx] = np.max(power_channel[:, beta_freq_indices, :])\n",
    "    itp_values[channel_idx] = np.max(power_channel[:, theta_freq_indices, :])\n",
    "    igp_values[channel_idx] = np.max(power_channel[:, gamma_freq_indices, :])\n",
    "\n",
    "    # Calculate slope\n",
    "    log_freqs = np.log(freqs)\n",
    "    log_power = np.log(power_channel + epsilon) #small offset to remove log0 problem\n",
    "\n",
    "    log_freqs_alpha = log_freqs[alpha_freq_indices]\n",
    "    log_power_alpha = log_power[:, :, alpha_freq_indices]\n",
    "\n",
    "    log_power_alpha_reshaped = log_power_alpha.reshape(-1, log_power_alpha.shape[-1])\n",
    "\n",
    "    X = np.column_stack((np.ones(log_freqs_alpha.shape), log_freqs_alpha))\n",
    "    results = np.linalg.lstsq(X, log_power_alpha_reshaped.T, rcond=None)\n",
    "    slope = results[0][1]\n",
    "\n",
    "    slope_values[channel_idx] = slope[0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for channel_idx in range(num_channels):\n",
    "    #Calculate Theta/Alpha ratio for each channel\n",
    "    theta_alpha_ratio[channel_idx] = mean_psd_theta_channels[channel_idx] / mean_psd_alpha_channels[channel_idx]\n",
    "        #Calculate Theta/Alpha ratio for each channel\n",
    "    theta_gamma_ratio[channel_idx] = mean_psd_theta_channels[channel_idx] / mean_psd_gamma_channels[channel_idx]\n",
    "\n",
    "    \n",
    "for channel_idx in range(num_channels):\n",
    "    power_channel = power_values[:, channel_idx, :, :]\n",
    "\n",
    "    # Calculate variance and entropy for each channel separately across all epochs\n",
    "    variance_per_channel[channel_idx] = np.mean(np.var(power_channel, axis=(0, 2)))\n",
    "    entropy_per_channel[channel_idx] = -np.mean(np.sum(power_channel * np.log(power_channel+epsilon), axis=(0, 2)))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Prepare results for DataFrame\n",
    "results_dict = {\n",
    "    'Slope': slope_values,\n",
    "    'ITF': itf_values,\n",
    "    'IAF': iaf_values,\n",
    "    'IGF': igf_values,\n",
    "    'IBF': ibf_values,\n",
    "    'IAP': iap_values,\n",
    "    'ITP': itp_values,\n",
    "    'IGP': igp_values,\n",
    "    'IBP': ibp_values,\n",
    "    'Mean_PSD_Alpha': mean_psd_alpha_channels,\n",
    "    'Mean_PSD_Beta': mean_psd_beta_channels,\n",
    "    'Mean_PSD_Gamma': mean_psd_gamma_channels,\n",
    "    'Mean_PSD_Theta': mean_psd_theta_channels,\n",
    "    'Theta_Alpha_Ratio': theta_alpha_ratio,\n",
    "    'Theta_Gamma_Ratio': theta_gamma_ratio,\n",
    "    'Variance': variance_per_channel,\n",
    "    'Entropy': entropy_per_channel\n",
    "}\n",
    "\n",
    "\n",
    "# Define the path for the CSV file\n",
    "results_csv_file = 'Frameworkfeatures2.csv'\n",
    "\n",
    "# Extract the filename from the eeg_file_path\n",
    "filename = os.path.basename(eeg_file_path)\n",
    "\n",
    "# Create a dictionary to store data for each column\n",
    "data_dict = {'Filename': filename, 'EEG_State': eeg_state_label, 'Level': eeg_level_label} \n",
    "\n",
    "# Append EEG channel features to the DataFrame\n",
    "for channel_idx in range(num_channels):\n",
    "    for feature_name, feature_value in results_dict.items():\n",
    "        # Construct column name as feature name + EEG channel name\n",
    "        column_name = f'{feature_name}_channel{channel_idx+1}'\n",
    "        data_dict[column_name] = feature_value[channel_idx]\n",
    "\n",
    "# Append the data to the DataFrame\n",
    "results_df = pd.DataFrame(data_dict, index=[0])\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if not os.path.exists(results_csv_file):\n",
    "    # If the file does not exist, create a new file and write the DataFrame\n",
    "    results_df.to_csv(results_csv_file, index=False)\n",
    "    print(\"Results saved to:\", results_csv_file)\n",
    "else:\n",
    "    # If the file already exists, append the new data to the existing file\n",
    "    results_df.to_csv(results_csv_file, mode='a', header=False, index=False)\n",
    "    print(\"Results appended to:\", results_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5099c32b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b04406ab",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matlpotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatlpotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'matlpotlib'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matlpotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c449e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "df = pd.read_csv(\"Frameworkfeatures2.csv\")\n",
    "\n",
    "# Show EEG channel names\n",
    "print(\"EEG channel names:\")\n",
    "for idx, channel in enumerate(df.columns):\n",
    "    print(f\"  {idx + 1}. {channel}\")\n",
    "\n",
    "# Function to get channel pairs for a specific region\n",
    "def get_channel_pairs(region_name):\n",
    "    pairs_input = input(f\"Enter EEG channel pairs for {region_name} region (channel names separated by comma) (e.g., 'Fz,Cz P3,P4'): \")\n",
    "    return [tuple(pair.split(',')) for pair in pairs_input.split()]\n",
    "\n",
    "# Dictionary to store region names and their respective channel pairs\n",
    "region_pairs = {\n",
    "    \"Frontal\": [],\n",
    "    \"Parietal\": [],\n",
    "    \"Central\": [],\n",
    "    \"Temporal\": [],\n",
    "    \"Occipital\": []\n",
    "}\n",
    "\n",
    "# Get channel pairs for each region\n",
    "for region_name in region_pairs.keys():\n",
    "    region_pairs[region_name] = get_channel_pairs(region_name)\n",
    "\n",
    "# List of features to process\n",
    "features_to_process = [\n",
    "    \"Slope\", \"ITF\", \"IAF\", \"IGF\", \"IBF\", \"IAP\", \"ITP\", \"IGP\", \"IBP\",\n",
    "    \"Mean_PSD_Alpha\", \"Mean_PSD_Beta\", \"Mean_PSD_Gamma\", \"Mean_PSD_Theta\",\n",
    "    \"Theta_Alpha_Ratio\", \"Theta_Gamma_Ratio\", \"Variance\", \"Entropy\"\n",
    "]\n",
    "\n",
    "# Iterate over features and channel pairs to compute mean values\n",
    "for feature in features_to_process:\n",
    "    for region_name, pairs in region_pairs.items():\n",
    "        if pairs:  # Check if channel pairs are provided for the region\n",
    "            for pair in pairs:\n",
    "                # Select channels for the pair\n",
    "                channels = [f\"{feature}_{channel}\" for channel in pair if f\"{feature}_{channel}\" in df.columns]\n",
    "\n",
    "                # If channels exist for the pair\n",
    "                if channels:\n",
    "                    # Filter out non-numeric values\n",
    "                    numeric_mask = df[channels].apply(pd.to_numeric, errors='coerce').notnull().all(axis=1)\n",
    "\n",
    "                    # Compute mean for all channels in the pair\n",
    "                    df[f\"Mean_{feature}_{region_name}\"] = df[channels].mean(axis=1)\n",
    "\n",
    "                    # Drop the original columns\n",
    "                    df.drop(channels, axis=1, inplace=True)\n",
    "\n",
    "# Save the modified DataFrame to a new CSV file\n",
    "df.to_csv(\"ModifiedFrameworkfeatures2.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Read the modified CSV file\n",
    "df = pd.read_csv(\"ModifiedFrameworkfeatures2.csv\")\n",
    "\n",
    "# Define conditions with names\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store mean values\n",
    "condition_means_df = pd.DataFrame(columns=list(df.columns) + ['Condition'])\n",
    "\n",
    "# Calculate mean for each condition\n",
    "for condition, (level, eeg_state, condition_name) in conditions.items():\n",
    "    # Filter rows based on condition\n",
    "    condition_df = df[(df['Level'] == level) & (df['EEG_State'] == eeg_state)]\n",
    "    # Calculate mean\n",
    "    condition_mean = condition_df.mean(axis=0)\n",
    "    # Set condition name\n",
    "    condition_mean['Condition'] = condition_name\n",
    "    # Append mean values to condition_means_df\n",
    "    condition_means_df = condition_means_df.append(condition_mean, ignore_index=True)\n",
    "\n",
    "# Drop the columns \"Filename\", \"EEG_state\", and \"Level\"\n",
    "condition_means_df.drop([\"Filename\", \"EEG_State\", \"Level\"], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns to have \"Condition\" as the first column\n",
    "cols = condition_means_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "condition_means_df = condition_means_df[cols]\n",
    "\n",
    "# Save the DataFrame with condition means to a new CSV file\n",
    "condition_means_df.to_csv(\"ConditionMeansFrameworkfeatures2.csv\", index=False)\n",
    "\n",
    "\n",
    "# Read the CSV file with condition means\n",
    "df = pd.read_csv(\"ConditionMeansFrameworkfeatures2.csv\")\n",
    "\n",
    "# Extract condition names\n",
    "conditions = df['Condition']\n",
    "\n",
    "# Drop the 'Condition' column\n",
    "df = df.drop(columns=['Condition'])\n",
    "\n",
    "\n",
    "# Set up plot grid\n",
    "num_features = len(df.columns)\n",
    "num_cols = 2  # Number of columns in the plot grid\n",
    "num_rows = (num_features + 1) // num_cols  # Calculate number of rows needed\n",
    "plt.figure(figsize=(15, 5 * num_rows))\n",
    "\n",
    "# Iterate over columns (features)\n",
    "for i, column in enumerate(df.columns, 1):\n",
    "    # Create a subplot for the current feature\n",
    "    plt.subplot(num_rows, num_cols, i)\n",
    "    \n",
    "    # Plot bar graph for the current feature\n",
    "    plt.bar(conditions, df[column], color=['blue', 'orange', 'green', 'red'])\n",
    "    \n",
    "    # Set plot title\n",
    "    plt.title(column)\n",
    "    \n",
    "    # Set plot properties\n",
    "    plt.xlabel('Conditions')\n",
    "    plt.ylabel('Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Show plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e0837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the modified CSV file\n",
    "df = pd.read_csv(\"ModifiedFrameworkfeatures.csv\")\n",
    "\n",
    "# Define conditions with names\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store max values\n",
    "condition_max_df = pd.DataFrame(columns=list(df.columns) + ['Condition'])\n",
    "\n",
    "# Calculate max for each condition\n",
    "for condition, (level, eeg_state, condition_name) in conditions.items():\n",
    "    # Filter rows based on condition\n",
    "    condition_df = df[(df['Level'] == level) & (df['EEG_State'] == eeg_state)]\n",
    "    # Calculate max\n",
    "    condition_max = condition_df.max(axis=0)\n",
    "    # Set condition name\n",
    "    condition_max['Condition'] = condition_name\n",
    "    # Append max values to condition_max_df\n",
    "    condition_max_df = condition_max_df.append(condition_max, ignore_index=True)\n",
    "\n",
    "# Drop the columns \"Filename\", \"EEG_state\", and \"Level\"\n",
    "condition_max_df.drop([\"Filename\", \"EEG_State\", \"Level\"], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns to have \"Condition\" as the first column\n",
    "cols = condition_max_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "condition_max_df = condition_max_df[cols]\n",
    "\n",
    "# Save the DataFrame with condition max values to a new CSV file\n",
    "condition_max_df.to_csv(\"ConditionMaxFrameworkfeatures.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c81d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "condition_means_df = pd.read_csv(\"ConditionMaxFrameworkfeatures.csv\", index_col=0)\n",
    "\n",
    "# Define the conditions\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Ask for input regarding the threshold percentage\n",
    "threshold_percentage = float(input(\"Enter the threshold percentage (between 0 and 100): \"))\n",
    "\n",
    "# Convert the threshold percentage to a scale factor\n",
    "threshold_scale_factor = threshold_percentage / 100\n",
    "\n",
    "# Min-Max scaling\n",
    "condition_means_df_scaled = (condition_means_df - condition_means_df.min()) / (condition_means_df.max() - condition_means_df.min())\n",
    "\n",
    "# Perform scaling without normalization\n",
    "condition_means_df_scaled *= 100  # Multiplying by 100 to keep values within a reasonable range\n",
    "\n",
    "# Calculate the range of values for each feature\n",
    "feature_ranges = condition_means_df_scaled.max() - condition_means_df_scaled.min()\n",
    "\n",
    "# Define specified conditions to compare\n",
    "specified_conditions = [\n",
    "    (\"Experienced Rest\", \"Experienced Meditation\"),\n",
    "    (\"Experienced Rest\", \"Novice Rest\"),\n",
    "    (\"Novice Rest\", \"Novice Meditation\"),\n",
    "    (\"Novice Meditation\", \"Experienced Meditation\")\n",
    "]\n",
    "\n",
    "# Find differences exceeding threshold or showing significant difference\n",
    "significant_differences = []  # Store unique combinations of differences\n",
    "for feature in condition_means_df.columns:\n",
    "    for cond1, cond2 in specified_conditions:\n",
    "        diff = condition_means_df_scaled.loc[conditions[cond1][2], feature] - condition_means_df_scaled.loc[conditions[cond2][2], feature]\n",
    "        threshold = feature_ranges[feature] * threshold_scale_factor\n",
    "        if abs(diff) >= threshold:\n",
    "            significant_differences.append((feature, cond1, cond2, round(diff, 2)))\n",
    "\n",
    "# Sort significant differences based on absolute difference in descending order\n",
    "significant_differences.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "\n",
    "# Display list of significant differences\n",
    "print(\"Significant Differences (Ordered by Absolute Difference):\")\n",
    "for feature, cond1, cond2, diff in significant_differences:\n",
    "    print(f\"Feature: {feature}, Conditions: {cond1} vs {cond2}, Difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea1a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the modified CSV file\n",
    "df = pd.read_csv(\"ModifiedFrameworkfeatures.csv\")\n",
    "\n",
    "# Define conditions with names\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store max values\n",
    "condition_min_df = pd.DataFrame(columns=list(df.columns) + ['Condition'])\n",
    "\n",
    "# Calculate max for each condition\n",
    "for condition, (level, eeg_state, condition_name) in conditions.items():\n",
    "    # Filter rows based on condition\n",
    "    condition_df = df[(df['Level'] == level) & (df['EEG_State'] == eeg_state)]\n",
    "    # Calculate max\n",
    "    condition_min = condition_df.min(axis=0)\n",
    "    # Set condition name\n",
    "    condition_min['Condition'] = condition_name\n",
    "    # Append max values to condition_max_df\n",
    "    condition_min_df = condition_min_df.append(condition_min, ignore_index=True)\n",
    "\n",
    "# Drop the columns \"Filename\", \"EEG_state\", and \"Level\"\n",
    "condition_min_df.drop([\"Filename\", \"EEG_State\", \"Level\"], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns to have \"Condition\" as the first column\n",
    "cols = condition_min_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "condition_min_df = condition_min_df[cols]\n",
    "\n",
    "# Save the DataFrame with condition max values to a new CSV file\n",
    "condition_min_df.to_csv(\"ConditionMinFrameworkfeatures.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb30f084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "condition_min_df = pd.read_csv(\"ConditionMinFrameworkfeatures.csv\", index_col=0)\n",
    "\n",
    "# Define the conditions\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Ask for input regarding the threshold percentage\n",
    "threshold_percentage = float(input(\"Enter the threshold percentage (between 0 and 100): \"))\n",
    "\n",
    "# Convert the threshold percentage to a scale factor\n",
    "threshold_scale_factor = threshold_percentage / 100\n",
    "\n",
    "# Min-Max scaling\n",
    "condition_min_df_scaled = (condition_min_df - condition_min_df.min()) / (condition_min_df.max() - condition_min_df.min())\n",
    "\n",
    "# Perform scaling without normalization\n",
    "condition_min_df_scaled *= 100  # Multiplying by 100 to keep values within a reasonable range\n",
    "\n",
    "# Calculate the range of values for each feature\n",
    "feature_ranges = condition_min_df_scaled.max() - condition_min_df_scaled.min()\n",
    "\n",
    "# Define specified conditions to compare\n",
    "specified_conditions = [\n",
    "    (\"Experienced Rest\", \"Experienced Meditation\"),\n",
    "    (\"Experienced Rest\", \"Novice Rest\"),\n",
    "    (\"Novice Rest\", \"Novice Meditation\"),\n",
    "    (\"Novice Meditation\", \"Experienced Meditation\")\n",
    "]\n",
    "\n",
    "# Find differences exceeding threshold or showing significant difference\n",
    "significant_differences = []  # Store unique combinations of differences\n",
    "for feature in condition_min_df.columns:\n",
    "    for cond1, cond2 in specified_conditions:\n",
    "        diff = condition_min_df_scaled.loc[conditions[cond1][2], feature] - condition_min_df_scaled.loc[conditions[cond2][2], feature]\n",
    "        threshold = feature_ranges[feature] * threshold_scale_factor\n",
    "        if abs(diff) >= threshold:\n",
    "            significant_differences.append((feature, cond1, cond2, round(diff, 2)))\n",
    "\n",
    "# Sort significant differences based on absolute difference in descending order\n",
    "significant_differences.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "\n",
    "# Display list of significant differences\n",
    "print(\"Significant Differences (Ordered by Absolute Difference):\")\n",
    "for feature, cond1, cond2, diff in significant_differences:\n",
    "    print(f\"Feature: {feature}, Conditions: {cond1} vs {cond2}, Difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6d2f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the modified CSV file\n",
    "df = pd.read_csv(\"ModifiedFrameworkfeatures.csv\")\n",
    "\n",
    "# Define conditions with names\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store max values\n",
    "condition_median_df = pd.DataFrame(columns=list(df.columns) + ['Condition'])\n",
    "\n",
    "# Calculate max for each condition\n",
    "for condition, (level, eeg_state, condition_name) in conditions.items():\n",
    "    # Filter rows based on condition\n",
    "    condition_df = df[(df['Level'] == level) & (df['EEG_State'] == eeg_state)]\n",
    "    # Calculate max\n",
    "    condition_median = condition_df.median(axis=0)\n",
    "    # Set condition name\n",
    "    condition_median['Condition'] = condition_name\n",
    "    # Append max values to condition_max_df\n",
    "    condition_median_df = condition_median_df.append(condition_median, ignore_index=True)\n",
    "\n",
    "# Drop the columns \"Filename\", \"EEG_state\", and \"Level\"\n",
    "condition_median_df.drop([\"Filename\", \"EEG_State\", \"Level\"], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns to have \"Condition\" as the first column\n",
    "cols = condition_median_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "condition_median_df = condition_median_df[cols]\n",
    "\n",
    "# Save the DataFrame with condition max values to a new CSV file\n",
    "condition_median_df.to_csv(\"ConditionMedianFrameworkfeatures.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ac33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "condition_median_df = pd.read_csv(\"ConditionMedianFrameworkfeatures.csv\", index_col=0)\n",
    "\n",
    "# Define the conditions\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Ask for input regarding the threshold percentage\n",
    "threshold_percentage = float(input(\"Enter the threshold percentage (between 0 and 100): \"))\n",
    "\n",
    "# Convert the threshold percentage to a scale factor\n",
    "threshold_scale_factor = threshold_percentage / 100\n",
    "\n",
    "# Min-Max scaling\n",
    "condition_median_df_scaled = (condition_median_df - condition_median_df.min()) / (condition_median_df.max() - condition_median_df.min())\n",
    "\n",
    "# Perform scaling without normalization\n",
    "condition_median_df_scaled *= 100  # Multiplying by 100 to keep values within a reasonable range\n",
    "\n",
    "# Calculate the range of values for each feature\n",
    "feature_ranges = condition_median_df_scaled.max() - condition_median_df_scaled.min()\n",
    "\n",
    "# Define specified conditions to compare\n",
    "specified_conditions = [\n",
    "    (\"Experienced Rest\", \"Experienced Meditation\"),\n",
    "    (\"Experienced Rest\", \"Novice Rest\"),\n",
    "    (\"Novice Rest\", \"Novice Meditation\"),\n",
    "    (\"Novice Meditation\", \"Experienced Meditation\")\n",
    "]\n",
    "\n",
    "# Find differences exceeding threshold or showing significant difference\n",
    "significant_differences = []  # Store unique combinations of differences\n",
    "for feature in condition_median_df.columns:\n",
    "    for cond1, cond2 in specified_conditions:\n",
    "        diff = condition_median_df_scaled.loc[conditions[cond1][2], feature] - condition_median_df_scaled.loc[conditions[cond2][2], feature]\n",
    "        threshold = feature_ranges[feature] * threshold_scale_factor\n",
    "        if abs(diff) >= threshold:\n",
    "            significant_differences.append((feature, cond1, cond2, round(diff, 2)))\n",
    "\n",
    "# Sort significant differences based on absolute difference in descending order\n",
    "significant_differences.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "\n",
    "# Display list of significant differences\n",
    "print(\"Significant Differences (Ordered by Absolute Difference):\")\n",
    "for feature, cond1, cond2, diff in significant_differences:\n",
    "    print(f\"Feature: {feature}, Conditions: {cond1} vs {cond2}, Difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f2be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the modified CSV file\n",
    "df = pd.read_csv(\"ModifiedFrameworkfeatures.csv\")\n",
    "\n",
    "# Define conditions with names\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame to store max values\n",
    "condition_std_df = pd.DataFrame(columns=list(df.columns) + ['Condition'])\n",
    "\n",
    "# Calculate max for each condition\n",
    "for condition, (level, eeg_state, condition_name) in conditions.items():\n",
    "    # Filter rows based on condition\n",
    "    condition_std = df[(df['Level'] == level) & (df['EEG_State'] == eeg_state)]\n",
    "    # Calculate max\n",
    "    condition_std = condition_std.std(axis=0)\n",
    "    # Set condition name\n",
    "    condition_std['Condition'] = condition_name\n",
    "    # Append max values to condition_max_df\n",
    "    condition_std_df = condition_std_df.append(condition_std, ignore_index=True)\n",
    "\n",
    "# Drop the columns \"Filename\", \"EEG_state\", and \"Level\"\n",
    "condition_std_df.drop([\"Filename\", \"EEG_State\", \"Level\"], axis=1, inplace=True)\n",
    "\n",
    "# Reorder columns to have \"Condition\" as the first column\n",
    "cols = condition_std_df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "condition_std_df = condition_std_df[cols]\n",
    "\n",
    "# Save the DataFrame with condition max values to a new CSV file\n",
    "condition_std_df.to_csv(\"ConditionStdFrameworkfeatures.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "condition_std_df = pd.read_csv(\"ConditionStdFrameworkfeatures.csv\", index_col=0)\n",
    "\n",
    "# Define the conditions\n",
    "conditions = {\n",
    "    \"Experienced Rest\": (\"Experienced\", \"Rest\", \"experienced-rest\"),\n",
    "    \"Experienced Meditation\": (\"Experienced\", \"Meditation\", \"experienced-meditation\"),\n",
    "    \"Novice Rest\": (\"Novice\", \"Rest\", \"novice-rest\"),\n",
    "    \"Novice Meditation\": (\"Novice\", \"Meditation\", \"novice-meditation\")\n",
    "}\n",
    "\n",
    "# Ask for input regarding the threshold percentage\n",
    "threshold_percentage = float(input(\"Enter the threshold percentage (between 0 and 100): \"))\n",
    "\n",
    "# Convert the threshold percentage to a scale factor\n",
    "threshold_scale_factor = threshold_percentage / 100\n",
    "\n",
    "# Min-Max scaling\n",
    "condition_std_df_scaled = (condition_std_df - condition_std_df.min()) / (condition_std_df.max() - condition_std_df.min())\n",
    "\n",
    "# Perform scaling without normalization\n",
    "condition_std_df_scaled *= 100  # Multiplying by 100 to keep values within a reasonable range\n",
    "\n",
    "# Calculate the range of values for each feature\n",
    "feature_ranges = condition_std_df_scaled.max() - condition_std_df_scaled.min()\n",
    "\n",
    "# Define specified conditions to compare\n",
    "specified_conditions = [\n",
    "    (\"Experienced Rest\", \"Experienced Meditation\"),\n",
    "    (\"Experienced Rest\", \"Novice Rest\"),\n",
    "    (\"Novice Rest\", \"Novice Meditation\"),\n",
    "    (\"Novice Meditation\", \"Experienced Meditation\")\n",
    "]\n",
    "\n",
    "# Find differences exceeding threshold or showing significant difference\n",
    "significant_differences = []  # Store unique combinations of differences\n",
    "for feature in condition_std_df.columns:\n",
    "    for cond1, cond2 in specified_conditions:\n",
    "        diff = condition_std_df_scaled.loc[conditions[cond1][2], feature] - condition_std_df_scaled.loc[conditions[cond2][2], feature]\n",
    "        threshold = feature_ranges[feature] * threshold_scale_factor\n",
    "        if abs(diff) >= threshold:\n",
    "            significant_differences.append((feature, cond1, cond2, round(diff, 2)))\n",
    "\n",
    "# Sort significant differences based on absolute difference in descending order\n",
    "significant_differences.sort(key=lambda x: abs(x[3]), reverse=True)\n",
    "\n",
    "# Display list of significant differences\n",
    "print(\"Significant Differences (Ordered by Absolute Difference):\")\n",
    "for feature, cond1, cond2, diff in significant_differences:\n",
    "    print(f\"Feature: {feature}, Conditions: {cond1} vs {cond2}, Difference: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3567059e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd70cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabef5fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b7658a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48063ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cbfce3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fb2672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea3c89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5d0894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedd056c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69b82aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1931e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c9280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7992b71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cc61d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1826304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca544fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb95c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3336f374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364ab4c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
